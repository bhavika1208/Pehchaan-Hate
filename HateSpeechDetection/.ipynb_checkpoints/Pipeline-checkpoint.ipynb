{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ae3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, auc, average_precision_score, confusion_matrix, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5a0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_tweet.csv')\n",
    "test = pd.read_csv('data/test_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f857ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    result = re.sub(r'(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "    result = re.sub(r'(@[A-Za-z0-9-_]+)', '', result)\n",
    "    result = re.sub(r'http\\S+', '', result)\n",
    "    result = re.sub(r'bit.ly/\\S+', '', result) \n",
    "    result = re.sub(r'&[\\S]+?;', '', result)\n",
    "    result = re.sub(r'#', ' ', result)\n",
    "    result = re.sub(r'[^\\w\\s]', r'', result)    \n",
    "    result = re.sub(r'\\w*\\d\\w*', r'', result)\n",
    "    result = re.sub(r'\\s\\s+', ' ', result)\n",
    "    result = re.sub(r'(\\A\\s+|\\s+\\Z)', '', result)\n",
    "    result = tokenize(result)\n",
    "    return result \n",
    "\n",
    "def lemmatize(token):\n",
    "    return WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "\n",
    "def tokenize(tweet):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(tweet):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:  # drops stopwords and words with <3 characters\n",
    "            result.append(lemmatize(token))\n",
    "    result = ' '.join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6da2d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drag kid dysfunct...\n",
       "1    thank lyft credit use cause dont offer wheelch...\n",
       "2                                       bihday majesty\n",
       "3                         model love time urð ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed = train['tweet'].apply(lambda x: preprocess(x))\n",
    "train_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f668283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aps(X, y, model):\n",
    "    probs = model.decision_function(X)\n",
    "    return average_precision_score(y, probs)\n",
    "\n",
    "def auc(X, y, model):\n",
    "    probs = model.decision_function(X) \n",
    "    return roc_auc_score(y, probs)\n",
    "\n",
    "def get_metrics(X, y, y_pred, model):\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    rec = recall_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred)\n",
    "    rocauc = auc(X, y, model)\n",
    "    prauc = aps(X, y, model)\n",
    "    \n",
    "    print('Accuracy: ', acc)\n",
    "    print('F1: ', f1)\n",
    "    print('Recall: ', rec)\n",
    "    print('Precision: ', prec)\n",
    "    print('ROC-AUC: ', rocauc)\n",
    "    print('PR-AUC: ', prauc)\n",
    "\n",
    "def get_confusion(y, y_pred):\n",
    "    cnf = confusion_matrix(y, y_pred)\n",
    "    group_names = ['TN','FP','FN','TP']\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cnf.flatten()]\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cnf.flatten()/np.sum(cnf)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sns.heatmap(cnf, annot=labels, fmt='', cmap='Blues', annot_kws={'size':14}, cbar=False, xticklabels=False, yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a7c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_processed\n",
    "y = train.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c81798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613cac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr = train_processed.values\n",
    "# X_val = valid_processed.values\n",
    "# y_tr = train.label.values\n",
    "# y_val = valid.label.values\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "tfidf_tr = vec.fit_transform(x_train)\n",
    "tfidf_val = vec.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f58870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9638343136028031\n",
      "F1:  0.6681974741676234\n",
      "Recall:  0.5459662288930581\n",
      "Precision:  0.8609467455621301\n",
      "ROC-AUC:  0.9492637192291844\n",
      "PR-AUC:  0.7581136932034951\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty='l2', solver='lbfgs').fit(tfidf_tr, y_train)\n",
    "y_pred = clf.predict(tfidf_val)\n",
    "get_metrics(tfidf_val, y_valid, y_pred, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6c2b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADnCAYAAADy1tHpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAfYElEQVR4nO3dd1QUVxvH8S/LUlUEAbuiUbFhL1Hs0VhQrNhj773EroglYleMvRcEFXsv0WhiwRixl9i7SQREpUp9/0BXV0rMGwEvPp9z9hz3zp3hDo8/Zndn7qxBXFwcQgh1aNJ6AEKIf0dCK4RiJLRCKEZCK4RiJLRCKEab3EKzMv3lo2VFBf2+IK2HIP4DUy0GSS2TI60QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqEYCa0QipHQCqGYZL9UWlXh55P/QmXPXafp6bae8PMLiIyKplSzydx/Eqhbvmzid1hbZqTFoCUpPVTxD1zHjGLXzu0J2jdt2YGX51rdMq1WS7bs2aldpy59+g3A3Nw8tYeaatJlaPPVGa37d4PqDiwe316vLfx1lO7fMTGxTOzvTKfRa1JziOJfqFTZkSlTZ+i1WVpZ6S2Ljo7mnN9ZJrqNIzw8jHHjJ6bFUFNFunx5/HdgsO7xMjg8QdurkAhd38Ubf6HFt2UpUzRPWg1X/AMjY2NsbG31HlqtVm9Z9hw5cGrkjFNDZ44eOZLGI05Z6TK0/8bZqw/YceQCUwY1TeuhiE/AxNSU6Oiof+6osC8+tADjF+yiStkCfOtYNK2HIhJx6sRxKpUvo3v07dU90X6XL11i/97dVKxUOZVHmLrS5Xvaf+vuowBWbTvF5IFNOOz7R1oPR3ygbLnyjJ8wWffcxNRU9++3gY6JiSY6Opqa39Rm1BjXtBhmqpHQvuG+dB9Xd0+gjVP5tB6K+ICpmRl57ewSXfY20FojLba2WTEyMkrl0aU+eXn8hn9QCB7rjuDWtxEmxvK3TBVvA50zZ64vIrAgodUzz/MIJsZGONcsmdZDESJJEtr3hIZH4r5sH2amxmk9FCGSZBAXF5fkQrMy/ZNeKD5rQb8nf1WY+LyZajFIapkcaYVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMUoe+nPH3snYpfTOkH7/uNXaD5Qf/L6gnFt6daiCqPnbMfD8920ra7Nq9CqfjlKFcmNZSZzCjuN5+Gfz/XWHdGtHvWrFqNk4dxkMDPBrEz/lNkhkawVy5Ywf95cWrdtz5hx4wEoVbxwon1bt2nHGFe31BxeqlI2tFW/m4mh5t2prOw2mTnlPYKtP53X69esTmnKFc/L02cvEmzD3NSIw6f/YM+xS8wc7pLozzEx1rLz54v8evY2I7vX+6T7ID7OpYsX2LrFB3t7/ZAeOXZC7/nVq1cY2K83des3SM3hpTplQxsQFKL3vFNTR16FRrDtp3O6trw5rJg13AWn3vPZuaBvgm0s8D4GQNlieZP8OZMX7wXiwy9SX3BwMKNHDmPCpCksW7JIb5mNra3e82M/H8EuXz7KV6iYmkNMdenmPW3nppXZuPd3wiPiJ0AbGmpYO7UL01Yc4Ma9v9N4dOL/NWmCK3W+rcfX/zBHNjQ0hAP799LCpVUqjSztpIvQ1q5UhPy5bVi9/ZSuzbV3QwJfhLJ884lk1hSfs62bfXj08CH9Bwz6x7779+4lMjIK5ybNUmFkaUvZl8fv69rckbNX7nPp5hMAqpYrSIfGX/N1m2lpPDLx/7p/7y7z581h9TovjIz/eQLHti0+fFO7NlmyZEmF0aUt5UNra5WRRjVLMniqj66tRnl7sttYcO/QFF2bVmvID4Oa0L99TQrWT993NkgPLl64QFBQEC2aOuvaYmJi8Dv7O1t8NnL67AWM34T5j+vXuXr1CgMGD02r4aYq5UPbsUklXkdGs/mgn65tmc+vbD+s/yny7kX98Dngx6ptJ1N7iOL/UKt2HbY4OOi1uY0dTV67fHTr2UtvwvvWzZvImSsXlSo7pvYw04Tyoe3czJHNB/0ICXuta/MPCsH/g0+Xo6Jj+DvgFbcePNO1ZbPORDZrCwrlzQpA0QLZscxkxqO/ggh6FQZAnuxWWFmYY5cj/pxwSftcANx55E9oeGSK7tuXzMLCAgsLC702M3NzLDJnplAhe11beHg4+/bupnPX7hgYJDmbLV1ROrTVyxeiYN6sdBmz9v9av7tLNcb1dtI93zE//rRQj/GerN/9GwCufRrSoXElXZ/fNsXf9Lxu93kc97v1/w5dfCIHD+wjPDycJs2ap/VQUo1Mgk+nZBK82mQSvBDpiIRWCMVIaIVQjIRWCMVIaIVQzGcb2ozmJswc1oIb+ybx3HcOR9cMpdyb2TharYYfBjbhzKbRBJyazd1DU1jj3pk82a2S3eayid8Rfn5BgkfAqdmJ9m9Vvxzh5xewdV5vvfY2Dcpza/9knhybzvTv9U815LTNzB97J5I1S6b/sPfp06YNXjSo+w0VypSgTcvmnPM7m2TfJ08eU6p44QSPk8d/1eu30duLps4NqFi2JI0b1mP3zh16y31PncTZqR6OFcsyZtRwoiLfnVsPCw3FuUFdbt9W69TdZ3uedvH4djgUykV3V0+ePAuirVNF9i4ZQNkWPxAS/prSRfMwY+VBLt54TOaMZkwb2oydC/tSodVUYmJiE93msJlbcP1xp17bz6uHcuLcnQR98+Wyxn1wU06cu63Xbm2ZgUXj29HTbT33HgewbX4fjp25yf7jVwDwGN2KqcsP8Ox58Cf6TaQPB/bvY8Y0d8aMc6NM2XJs2uhN31492L5rLzly5kxyvUVLV1C4cBHd88yZM+v+7bPRG485s3CbOJkSJUtx+fIlJrmNI5OFBTVrfUNsbCyjRw6ja7eeOFatyrAhA9my2Ye27b8DYMGPHtRr4ETBgoVSbsdTwGd5pDU1MaJp7dK4zt/Jcb9b3H0UwJSl+7jzyJ8eLavxKiSCRn0WsOXQOW49eMbZqw/oP2UjRb/KQZH82ZPc7quQCL0vl/4qty1f5bFl9Xb9Sxu1Wg3rpnbBbeEe7j0O0FuWP5cNL0Mi2HLoHH7XHvLr7zcpkj8bAE1rl8Yioxlrd/h++l+K4jzXrqZxk2a0aNmKrwoUYPRYV2xtbfHZtCHZ9SwtLfW+TPr9yQN7du+ihUtLGjRsRO48eWjg1JAWLVuzeuVyAIKCggh6/pzWbdtRsGAhatT8hnt34/9AX750Cd9TJ+nZO+E868/dZxlaraEGrdaQiNfReu0Rr6NwLFMg0XUsMsR//eGLN5cffowuzR25evsppy/e02uf2M+ZB08D8XpzVdT7bj98hrmpEaUK58bKwpxyxe24fOspFhlNcR/clP4/bPzon/+liIqM5Pq1q1SuUkWvvbJjFS5eOJ/EWvGGDhpAzWqV6dS+DT8dPKC3LDIyEmMTE702UxMTrly+TFRUFFmyZMHW1hbfkyeIiIjg/Dk/ChUuTHR0NJMnjmfs+Am6SQcq+SxDGxL2mtMX7zKqez1y2mZGozGgjVMFvi6Zn+w2Fgn6G2kNmTa0GXt+ucyTRG4rkxiLjKY0/7YMq7ed0muvXakILvXKMWBK4uF7ERxOj/GerJjckeOew/Hac4bDvteZMqgpq7efwsYqAye8RnB+6zi6u1T91/ueHgW9CCImJgZraxu99izW1gQE+Ce6jrm5OUOHj2TmHA8WLl5GxUqVGTFsCHt2v3t741ilKju2b+XK5UvExcVx9cpltm3dQnR0FC9eBGFgYMCM2R4sW7KIZo2dKFKkKE2btWDt6pU4OJTA2tqaLh3b49ygLosXzk/R38Gn9Nm+p+06bh1LJ7TnzqEpREfHcOGPR/gcOEvponn0+hkaalg9pROZM5njMnjZR2+/rVNFDDUavPee0bVZW2Zg+aQOdBq9hhfB4Umuu+voJXYdvaR77lj6KyqWzMeoOdu4tGM83cd7cv3On5zxGY3vhbtcvf30X+x5+pXYBf1JXeRvZZWFTp276p4XdyjBi6Ag1qxaQSPnJgD07N2XgAB/On3Xlri4OLJYW+PcpClrVq1AozEE4r+/1ttnq247Dx88YMvmTWzasoNe3TvTsnVb6tVvQLvWLhR3KEH1GjU/4R6njM82tPceB1C3+zzMTY2xyGjKXwGv8JzWhftPAnV9DA01rJvameIFc1Kvxzyevwz96O13ae7IjiMXdLN5AIoVyEkO28zsW/LujouaNzePC/59HmVdpujNEoL4o/yPY9vQd5I3+XPbYGRkyNHfbgBw/Owtqpcv9MWH1srSCkNDwwRH1eeBgQmOvskpUbIUO3ds0z03NTVl0g9TcXWbxPPAQGxsbdm6eRMZMmTAyirxMwmTJ45nyPfD0Wg0XLt6lfoNGmJubk6NmrU489tpCe2nEBYRSVhEJJaZzKjjWJSxHvEvj7RaDZ7TulKsQA7q9ZjH34Ef/2ltBQc7ShXOzfCZW/Xa/a4+oJzLFL22Cf0aYWlhzuCpPnp/MN4a2b0ev/x+kzOX71PSPhdaQ0PdMiMjrd4dI79URsbGFC1WnNOnTlG33rs7Jfr6nqLOt3U/ejs3/rie4GZuAEZGRmTLHv8B5IH9+6heoxYaTcJ3fju2b8XMzIy69Rrw6tUrAKKj4z83iYqKwiDpa/Q/K59taOtULopGY8CNe39TII8t7kOacuv+M9bt8sXQUIP3jG6UK25Hi0FLiIuLI5t1/HnRlyERRLyOv7nbiskdAOju6qm37S7Nq3DrwbMEU+vCIiK5dudPvbYXweEYGhomaAco8lV22jSoQKW28be1ufngGdExMXR3qcr1O39Sq2Jhpi0/kGC9L1GHTl0YO2oEDiVKUrpMWTb7bMD/2TNatm4DwLy5s7ly+RLLV8VPs9y1YztarZYiRYuh0Rjwy7GjbNzgzeChw3TbvH//HpcvXaRkqdK8evkKz3WruX3rFpPdE95mKDAwkKWLFrJ6nRcQP1+3QMFCrFuzitrf1uWnQwcZOWpsKvwm/rvPNrSZM5oyaUBjcmWz5PnLMHYeuYDbwt1ER8eSN0cWnGuVAsB3wyi99d6fC5sne8L7BWU0N6FlvXJMXbb/P49x4bi2jJi9VTcBP+J1FN3GrcNjVCssMpoxfeVBzl17+J9/TnpQv4ETL18EsXzpYvz9n1GwkD0LlywjZ874mwoE+Pvz+NEjvXWWL13M0z+fYqjRYJcvHxN/mKJ7PwsQGxOL59o1PLh/D61WS4WKX7POawO5cuVO8PNnTJ1Ch85dyJ4jh67tB/dpuI4dzUbv9TRq3JQ6ddW4r7XMp02nZD6t2mQ+rRDpiIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMVIaIVQjIRWCMUk+6XSwa9j5UulFZVMWYUCLEw18qXSQqQXElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFCOhFUIxElohFKNN6wGkpAnjRrNn144E7V4+29iwfh17du2gd/+BdO/ZR7fs7O9n6N2tE4d/OYWllVUqjla8r0Kposkub9i4KRMmT9XrZ25ujl2+/HTu1pNv6tRN6SGmmXQdWoCKlSozyX26XpulZXwYTUxM8Fy9khYurbHKkiUthieSsP/Ir7p/n/j1GFMmjtdrMzUx1f17rNskqlavSUhwMJ5rVjJ6+BCWr1lPyVJlUnXMqSXdvzw2NjbGxsZW76HVxv+tKlehIjly5mLFssVpPErxoffrlSmTRYK2jJky6fpmymSBjY0t+fJ/xehxEzA2NuHXY0fTaugpLt2HNjkajYb+g4ey1WcTjx89TOvhiE9Aa2SEodaQ6OjotB5Kikn3ofU9eYJqX5fTPQb26am3vGq1GpQqU4aF8z3SZoDik4mMjGTlssWEhoRQ4etKaT2cFJPu39OWKVuesW4Tdc9N3nsv9NbAIcPo8l0brnW6kppDE5+I29iRTHQdw+vXEWTMmIlBQ0dQpWr1tB5Wikn3oTU1MyVPXrtk+xR3KME3deoyf+5suvXqk2xf8fkZOGQ4latUJUOGjGSxtk7r4aS4dP/y+GP1GziY8+f88D15PK2HIv4laxsb8uS1+yICCxJanTx57Wjm0pKNXp5pPRQhkiWhfU+PXn0xNDRM62EIkSyDuLi4JBcGv45NeqH4rCVTVqEAC1ONQVLL5EgrhGIktEIoRkIrhGIktEIoRkIrhGLS1RVRq1cs4+iRn3hw/x5GxsaUKFGKfoOGULCQfaL9p0wcz/atmxk0dDgdOncF4OXLFyxdtIDffE/x159PsbS0omr1GvQZMEg3pU98eqtXxtfu4ZvaOZQoRb+B+rULDAxgvsdsfvM9SXBwMGXKlmf4qLHktcun67Ntiw+H9u/lxo3rhAQHs3PfYXLmypUGe5Ry0tWR1u/sGVxat2XlOm+WLF+DodaQfj278fLliwR9Dx86yNWrV7DNmlWv3f/ZM/yf/c3AIcPYuHUnk6ZO5/y5s4wdOSyV9uLLdO73M7i0asuKtd4sXr4GraEh/Xq9q11cXBzDB/fn0cMHzJq7gPWbtpEjR0769epKeFiYbjsREeF87ViFHr37pdGepLx0fZ42LCyUmo4VmeWxgOo1a+na/3z6hK4d27Fo2SoG9u1JqzbtdUfaxJw4/gtD+vfh6MkzZMyYMTWG/p+pfp42LCyUWlUqMnNufO0e3L+HSxMnvHy2Y1+4CACxsbHU/6YafQcOpmnzlnrrX7t6hU7tWip7pP1iz9OGhYYSGxuLhYWFri06OpqxI4fRrUdv8n9V4KO2ExoSgrGxMaamCWcIiZTxYe2ioqIAMDYx0fXRaDQYGRtz4fy5NBljWknXoZ01fSr2RYpSolRpXdvSRQvInNkSl9ZtP2obwa9esWThjzRt0VJ3xwuR8mbPmIp94Xe1y5cvPzly5mTRj3N5+fIFUVGRrF21nGd//0Wgv3/aDjaVpdv/hXNmTuPCeT9WrPXSXU/sd/YMe3Ztx9tn+0dtIzwsjCED+mKbNRsDh8h72tQy903tlq95VzutkRHTZ//I5AnjqFO9MoaGhlT4ujKOVaul8WhTX7oM7ewZUzl0YB9LV64ld+48uvazZ84Q4O9P/drvJkjHxMQw32M2G9avY9/hY7r2sLBQBvXtBYDHgsWYvPeyTKScOTPja7dkhX7tAIoWK463z3ZCgoOJiorCKksWOrdvTdHixdNotGkj3YV21jT3+MCuWku+/F/pLWvZui21v9W/teaAPj2oV78hTVu8+yAjNDSUgX17EhcXx/zFyzE3z5AqY//SzZruzk8H9rFkZcLave/tTd0ePrjP9WtX6N1vYGoN8bOQrkI7fcok9u3ZxSyPBWSysCAgIP69jrm5OebmGchibZ1gorRWq8XaxoZ8+fMD8YHt36sboaEhzPJYQHh4GOHh8acUMmfOjJGRceru1Bdiuvsk9u/Zxcy5idcO4PChA1haWpE9Z07u3LrJ7Bnu1KhVm0qOVXTbCQjwJzAggIcP7gNw7+5tgoNfkT1HDjJntkzt3UoR6eqUT/mSid/gukfvfvTq2z/RZc71a+ud8nl7s/LELFm5lvIVKn6awaYw1U75JHVz8h69+9GzT3ztNnp54rl2Fc8DA7GxtcGpURO69+qj94d02eIFLF+yMMF2xk9yx7lJs5QZfApI7pRPugqteEe10Ap9X+x5WiHSIwmtEIqR0AqhGAmtEIqR0AqhGGVC67PRizYtmlCjcnlqVC5Pl+/acOLXY8muc/vmTXp26UCVCqVpUKcGy5cs5P1Py/3OnqFrh7bUrlaJKhVK06KxE55rVult47TvSZo716dG5fK4jhlBVFSkbllYWCjNGtXjzu1bn3Rf0xufjV60dWlCTcfy1HQsT9cO/1y7uLg4vNevxaWJE47lS1K/djXme8xOtO+Fc35UKutA6+bOeu2/+Z6khXN9ajqWZ3witWvurGbtlLm4Imu27AwY/D157eyIjY1lz66dfD94AOs3bqGQfeEE/UNCQujXqxtlypVnrbcPD+7fZ6LraMzMzPmuUxcAzM0z0LrddxQsZI+pqRkXL5zDfdIETE1NadmmHbGxsbiOGkGnbj2o7FiFkd8PZtuWzbRu2x6AxfPnUbe+EwUKFkrNX4VysmXLTv/B35M3b3zt9u7eybAhA/DckHjtADxmTefEr8cYOHQ4BQrZExocrLvg4n2vXr3EbdwoKlSsxLNnf+vaY2NjcR09gk5de1DJsQqjhg1m+5bNtHpbuwXz+LaemrVTJrQ1a9XWe95v4GC2+mzk0sULiRb+wN7dRESEM+GHqZiamlKwkD337t3By3MN7Tt2xsDAgKLFilO02LvrVnPlzs3RIz9x/pwfLdu040VQEEFBz2nZui0mJiZUr1mL+3fvAHDl8iVO+57Cy2dbyu54OlDjg9r1HRBfu8tJ1O7+/Xts2ujFhs079KZPJhbvyW7jaNS4KXFxcRz56aCu/W3tXN7WrkYt7t2Lr93Vy5f4zfcU6zepWTtlXh6/LyYmhoP79xIWFkapJL7t+9LFC5QuW05vDmxlx6r4P3vG0ydPEl3nj+vXuHThAmXLVwDAKksWbGxtOX3qJBEREVw450dB+8JER0fjPnkCo8aNx9hYLmv8N2JiYjj0pnYlSydeu1+PHiFXrtz4njxOE6dvadygNhPGjeJ5YKBev82bvAkMDKBrj94JtqGrnW987c6f96NQoXe1GzlW3dopc6SF+PeoXTq0JTLyNWbm5szy+JGC9onf/ykwMICs2bLrtVm/ue44MNCfXLlz69qd6tQkKOg5MTEx9OjdF5dWbQAwMDBg2sy5zJk5jdnT3alSrTpNmjbHc80qihd3wNrahh6dvyMgwJ/6Ts5JXiop4Patm3R9r3Yz5/6Y5L27njx+zF9/PuXQgf24TXbHwMCAebNnMnRgX1Z5bkCj0XD71k1WLFnEKs+NiX6Vi4GBAVNnzGXOrGnMme6OY7XqNG7aHM+1qyjmEF+7nl3e1e7tpZIqUCq0dvnz4b15G8HBwfx8+BBu40azdOXaJItv8MGFYG8/hDJAf8HyNesJDwvj8qULzPeYQ85cuWno3ASA0mXLsW7DZl3fRw8fsH2LD14+2+jbsystWrXh23oN6Ni2JcUdHKhavean2+F0xC5fPrx83tVugutolqxIvHaxcbFERkYycco07PLFT+SYOGUaLk2cuHblMvZFijJmxFAGDR2u98f3Q6XLlmOdt37tdmzxYf2mbfTr1ZUWLdtQp14DOrVrSbHi6tROqZfHRkbG5MlrR7HiDvQfNJTChYvgvX5ton2trW0IDAjQa3v+/DkAWaxt9Npz5c5NQXt7mrm0ol3HTixbnPCC87fcJ01g4NBhGGg0XL92lXr1nciQIQPVa9Ti9zO//bcdTMc+rJ194SJsSKJ2Nja2GGq1usAC5LXLh6FWy19//UmAvz/37t5hkttYKpV1oFJZB1YsXcTdO7epVNaB06dOJrpd98kTGDDkXe3qvqldNcVqp9SR9kOxsXFERUYluqxkqdLM95jN69evdRPYf/M9hW3WrMne6CsuNpaoyMhEl+3asQ1TMzPq1K1P8KtXQPw9pyD+HkYfHtlF0uJi44hMonalSpclJjqax48ekjtPXgCePH5ETHQ0OXLkJGvWrGzYslNvnS0+Gzhz+hQz5sxPtL67dmzDLJ3UTpkj7XyP2Zz3O8vTJ0+4ffMmC+bNwe/sGeo7NQJgwbw59OneRde/vlMjTE3NmOA6mtu3bvLz4UOsXbWc9h3iPzkG2Oi9nuO/HOXhg/s8fHCfHdu2sH7taho0ck7w858HBrJ8yUJGjnUFIJOFBV8VKMj6dav54/o1jhw+SKky5VLhN6Ge+R6zOX/uTe1uvVe7hu/Vrse72lWsVJkiRYsxyW0sN65f48b1a0xyG4tDiZIULe6A1siIgoXs9R5ZslhjZGRMwUL2CW5a8DwwkBVLFzJiTMLa3bh+jZ9/OkhphWqnzJE2MCAA1zEjCAwIIGPGTBSyt+fHRcuoXKUqAAH+/jx+/FDXP2OmTCxcupLp7pPo2LYlmSwsaN+xC+07dtb1iX1zq5mnT55iqDUkd+489B80lBZvPoh636zp7rTv2IXs2XPo2iZOmcaEcaPZtMGLhs5NEtwVQ8QLDAxg/Hu1K2hvz7yF79UuwJ8n79VOo9Ewd/5iZk13p2fXDpiYmFKxsiNDho1Eo/n3x5nZM9xp30G/dhN+mMZE19H4bPSiYaMmfFNHndrJfNp0SubTqk3m0wqRjkhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxUhohVCMhFYIxST7TfBCiM+PHGmFUIyEVgjFSGiFUIyEVgjFSGiFUIyEVgjF/A+FCVu2DIFTDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_confusion(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b417990",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open(\"model1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b598814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vec, open(\"vec1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "523f2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(tweet):\n",
    "    model = pickle.load(open(\"model.pickle\", \"rb\"))\n",
    "    processed = preprocess_tweet(tweet)\n",
    "    lst = []\n",
    "    lst.append(processed)\n",
    "    vec = pickle.load(open(\"vec.pickle\", \"rb\"))\n",
    "    vectorized = vec.transform(lst)\n",
    "    pred = model.predict(vectorized)\n",
    "    prob = model.predict_proba(vectorized)[:,1]\n",
    "    mapping = {0: 'Same tweet, different day. Keep it movin\\'.', 1: 'Didn\\'t your parents ever wash your mouth out with SOAP?  Well they should!'}\n",
    "    prediction = mapping[pred[0]]\n",
    "    probability = str(prob)[1:-1]\n",
    "    return tweet, prediction, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2831c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"@canelo28969897 @ItsCandyyyyyyy @DiamondRhona @darrel30901325 @Noblenosey Don’t nobody want no damn white man ew &amp;… https://t.co/cuzxJAzFGL\" \n",
    "str2 = \"@user lol speak for your own hazara(mongol) people who have nothing to do with ethnic afghans (nor genetical\\u2026 @URL\"\n",
    "str3 = \"i am going to the store to get some broccoli and cheese, any suggestions on cheese?\"\n",
    "str4 = \"@canelo28969897 @ItsCandyyyyyyy @DiamondRhona @darrel30901325 @Noblenosey Don’t nobody want no damn white man ew fuck you hate you black people\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "94331dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@canelo28969897 @ItsCandyyyyyyy @DiamondRhona @darrel30901325 @Noblenosey Don’t nobody want no damn white man ew &amp;… https://t.co/cuzxJAzFGL',\n",
       " \"Didn't your parents ever wash your mouth out with SOAP?  Well they should!\",\n",
       " '0.77816142')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7a33a3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@user lol speak for your own hazara(mongol) people who have nothing to do with ethnic afghans (nor genetical… @URL',\n",
       " \"Same tweet, different day. Keep it movin'.\",\n",
       " '0.11098109')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b27147c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i am going to the store to get some broccoli and cheese, any suggestions on cheese?',\n",
       " \"Same tweet, different day. Keep it movin'.\",\n",
       " '0.01987867')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fd6a3dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@canelo28969897 @ItsCandyyyyyyy @DiamondRhona @darrel30901325 @Noblenosey Don’t nobody want no damn white man ew fuck you hate you black people',\n",
       " \"Didn't your parents ever wash your mouth out with SOAP?  Well they should!\",\n",
       " '0.97820886')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(str4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "295a23a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Black niggas be so motherfuckers hate them and fuck them ewwwwww white people are the better ones',\n",
       " \"Didn't your parents ever wash your mouth out with SOAP?  Well they should!\",\n",
       " '0.81481257')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(\"Black niggas be so motherfuckers hate them and fuck them ewwwwww white people are the better ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f37ee3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('You so ugly I want you to take your fat ass from here and go die you black nigga',\n",
       " \"Didn't your parents ever wash your mouth out with SOAP?  Well they should!\",\n",
       " '0.53135252')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(\"You so ugly I want you to take your fat ass from here and go die you black nigga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0e2cd6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i love you', \"Same tweet, different day. Keep it movin'.\", '0.00158434')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(\"i love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0557910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b6d1f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize1(tweet):\n",
    "    \"\"\"Returns tokenized representation of words in lemma form excluding stopwords\"\"\"\n",
    "    result = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    for token in word_tokens:\n",
    "        print(token.type())\n",
    "        if token.lower not in stop_words and len(token) > 2:  # drops words with less than 3 characters\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f0c5be22",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-d7447441e60e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"i am shy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-108-e90f17f119c4>\u001b[0m in \u001b[0;36mtokenize1\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mword_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# drops words with less than 3 characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "res = tokenize1(\"i am shy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c306c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a98e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63e92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1075fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e483b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328c7115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9991656585040257\n",
      "Validation Accuracy : 0.9526967838818671\n",
      "F1 score : 0.615071283095723\n",
      "[[7311  121]\n",
      " [ 257  302]]\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "train = pd.read_csv('Twitter-Sentiment-Analysis-master/train_tweet.csv')\n",
    "test = pd.read_csv('Twitter-Sentiment-Analysis-master/test_tweets.csv')\n",
    "\n",
    "# collecting the hashtags\n",
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    \n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    return hashtags\n",
    "\n",
    "# extracting hashtags from non racist/sexist tweets\n",
    "HT_regular = hashtag_extract(train['tweet'][train['label'] == 0])\n",
    "\n",
    "# extracting hashtags from racist/sexist tweets\n",
    "HT_negative = hashtag_extract(train['tweet'][train['label'] == 1])\n",
    "\n",
    "# unnesting list\n",
    "HT_regular = sum(HT_regular,[])\n",
    "HT_negative = sum(HT_negative,[])\n",
    "\n",
    "# tokenizing the words present in the training set\n",
    "tokenized_tweet = train['tweet'].apply(lambda x: x.split()) \n",
    "\n",
    "# creating a word to vector model\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            vector_size=200, # desired no. of features/independent variables \n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34)\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(train['tweet']), epochs=20)\n",
    "\n",
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(TaggedDocument(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "\n",
    "# label all the tweets\n",
    "labeled_tweets = add_label(tokenized_tweet)\n",
    "\n",
    "train_corpus = []\n",
    "\n",
    "for i in range(0, 31962):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', train['tweet'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "  \n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # stemming\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "\n",
    "    # joining them back with space\n",
    "    review = ' '.join(review)\n",
    "    train_corpus.append(review)\n",
    "    \n",
    "# creating bag of words\n",
    "cv = CountVectorizer(max_features = 2500)\n",
    "x = cv.fit_transform(train_corpus).toarray()\n",
    "y = train.iloc[:, 1]\n",
    "\n",
    "# splitting the training data into train and valid sets\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# standardization\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_valid = sc.transform(x_valid)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "\n",
    "print(\"Training Accuracy :\", model.score(x_train, y_train))\n",
    "print(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"F1 score :\", f1_score(y_valid, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d049632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffae7148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"model.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a364f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.save(\"model_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d1ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_w2v, open(\"vec.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5fe8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "607b0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(token):\n",
    "    \"\"\"Returns lemmatization of a token\"\"\"\n",
    "    return WordNetLemmatizer().lemmatize(token, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c62983c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    result = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    for token in word_tokens:    \n",
    "        if token.lower not in stop_words and len(token) > 2:  # drops words with less than 3 characters\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0a4495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    result = re.sub(r'(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "    result = re.sub(r'(@[A-Za-z0-9-_]+)', '', result)\n",
    "    result = re.sub(r'http\\S+', '', result)\n",
    "    result = re.sub(r'bit.ly/\\S+', '', result) \n",
    "    result = re.sub(r'&[\\S]+?;', '', result)\n",
    "    result = re.sub(r'#', ' ', result)\n",
    "    result = re.sub(r'[^\\w\\s]', r'', result)    \n",
    "    result = re.sub(r'\\w*\\d\\w*', r'', result)\n",
    "    result = re.sub(r'\\s\\s+', ' ', result)\n",
    "    result = re.sub(r'(\\A\\s+|\\s+\\Z)', '', result)\n",
    "    processed = tokenize(result)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76160fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(str):\n",
    "    processed_tweet = preprocess_tweet(str)\n",
    "    #vec = gensim.models.Word2Vec.load(\"model_w2v.model\")\n",
    "    #vec = pickle.load(open(\"vec.pickle\", \"rb\"))\n",
    "    vec = pickle.load(open(\"vec.pickle\", \"rb\"))\n",
    "    vectorized_tweet = vec.transform(processed_tweet)\n",
    "    #vectorized_tweet = vec.transform(processed_tweet)\n",
    "    model = pickle.load(open(\"model.pickle\", \"rb\"))\n",
    "    pred = model.predict(vectorized_tweet)\n",
    "    prob = model.predict_proba(vectorized_tweet)[:,1]\n",
    "    mapping = {0: 'Same tweet, different day. Keep it movin\\'.', 1: 'Didn\\'t your parents ever wash your mouth out with SOAP?  Well they should!'}\n",
    "    prediction = mapping[pred[0]]\n",
    "    probability = str(prob)[1:-1]\n",
    "    return tweet, prediction, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02d5a3d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 20249 features, but DecisionTreeClassifier is expecting 2500 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-54a41e87f41e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fuck you\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-293f043b3de4>\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(str)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#vectorized_tweet = vec.transform(processed_tweet)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorized_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorized_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Same tweet, different day. Keep it movin\\'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Didn\\'t your parents ever wash your mouth out with SOAP?  Well they should!'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m--> 630\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[1;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\",\n\u001b[0m\u001b[0;32m    403\u001b[0m                                     reset=False)\n\u001b[0;32m    404\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ensure_2d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    366\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
      "\u001b[1;31mValueError\u001b[0m: X has 20249 features, but DecisionTreeClassifier is expecting 2500 features as input."
     ]
    }
   ],
   "source": [
    "arr = pipeline(\"fuck you\")\n",
    "print(arr[0])\n",
    "print(arr[1])\n",
    "print(arr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(tweet):\n",
    "    model = pickle.load(open(\"static/clf.pickle\", \"rb\"))\n",
    "    processed = preprocess_tweet(tweet)\n",
    "    lst = []\n",
    "    lst.append(processed)\n",
    "    vec = pickle.load(open(\"static/vec.pickle\", \"rb\"))\n",
    "    vectorized = vec.transform(lst)\n",
    "    pred = model.predict(vectorized)\n",
    "    prob = model.predict_proba(vectorized)[:,1]\n",
    "    mapping = {0: 'Same tweet, different day. Keep it movin\\'.', 1: 'Didn\\'t your parents ever wash your mouth out with SOAP?  Well they should!'}\n",
    "    prediction = mapping[pred[0]]\n",
    "    probability = str(prob)[1:-1]\n",
    "    return tweet, prediction, probability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
